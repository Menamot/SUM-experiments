{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salva\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([26, 17, 16, ...,  5, 22, 11])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from DMC_class import * \n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "k=50\n",
    "t=30\n",
    "n_samples=10000\n",
    "X, y = make_classification(n_features=15,n_samples=n_samples,random_state=42,n_classes=k,n_informative=8)\n",
    "XD=KMeans(n_clusters=t).fit(X).labels_\n",
    "def checkarray(A,B):\n",
    "    bandera=0\n",
    "    for i in range(len(A)):\n",
    "        if np.abs(A[i]-B[i])>0.0001:\n",
    "            print(A[i],B[i])\n",
    "            bandera=1\n",
    "            print(\"error\")\n",
    "    if bandera==0:\n",
    "        print(\"ok\")\n",
    "\n",
    "def checkMatriz(A,B):\n",
    "    bandera=0\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(A[0])):\n",
    "            if A[i,j]!=B[i,j]:\n",
    "                bandera=1\n",
    "                print(\"errror\")\n",
    "    if bandera==0:\n",
    "        print(\"ok\")\n",
    "XD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 24, 30, ...,  4,  9, 40])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008989810943603516\n",
      "0.0010006427764892578\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "def compute_pHatOrginal(XD: np.ndarray, y: np.ndarray, K: int, T: int):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    XD : ndarray of shape (n_samples,)\n",
    "        Labels of profiles for each data point\n",
    "\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Labels\n",
    "\n",
    "    K : int\n",
    "        Number of classes\n",
    "\n",
    "    T : int\n",
    "        Number of profiles\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pHat : ndarray of shape(K, n_profiles)\n",
    "    \"\"\"\n",
    "    pHat = np.zeros((K, T))\n",
    "\n",
    "    for k in range(K):\n",
    "        Ik = np.where(y == k)[0]\n",
    "        mk = len(Ik)\n",
    "        for t in range(T):\n",
    "            pHat[k, t] = np.sum(XD[Ik] == t) / mk\n",
    "    return pHat\n",
    "\n",
    "def compute_pHatOpti(XD: np.ndarray, y: np.ndarray, K: int, T: int):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    XD : ndarray of shape (n_samples,)\n",
    "        Labels of profiles for each data point\n",
    "\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Labels\n",
    "\n",
    "    K : int\n",
    "        Number of classes\n",
    "\n",
    "    T : int\n",
    "        Number of profiles\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pHat : ndarray of shape(K, n_profiles)\n",
    "    \"\"\"\n",
    "    pHat = np.zeros((K, T))\n",
    "\n",
    "    for k in range(K):\n",
    "        Ik = np.where(y == k)[0]\n",
    "        mk = len(Ik)\n",
    "        \n",
    "        pHat[k] = np.bincount(XD[Ik], minlength=T)/mk\n",
    "    return pHat\n",
    "\n",
    "start=time.time()\n",
    "B=compute_pHatOrginal(XD,y,k,t)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "A=compute_pHatOpti(XD,y,k,t)\n",
    "print(time.time()-start)\n",
    "checkMatriz(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8209950923919678\n",
      "0.005000114440917969\n",
      "0.0010666847229003906\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred=np.random.randint(0, k, size=n_samples)\n",
    "L=np.ones((k, k)) - np.eye(k)\n",
    "\n",
    "def compute_conditional_riskWenlong(y_true: np.ndarray, y_pred: np.ndarray, K: int, L: np.ndarray):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : ndarray of shape (n_samples,)\n",
    "        Real labels\n",
    "\n",
    "    y_pred : ndarray of shape (n_samples,)\n",
    "        Predicted labels\n",
    "\n",
    "    K : int\n",
    "        Number of classes\n",
    "\n",
    "    L : ndarray of shape (K, K)\n",
    "        Loss matrix, where K is the number of unique classes\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : ndarray of shape (K,)\n",
    "        Conditional risk\n",
    "\n",
    "    confmat : ndarray of shape (K, K)\n",
    "        Confusion matrix\n",
    "    \"\"\"\n",
    "    # Identify all unique classes\n",
    "    unique_classes = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "    # Initialize the confusion matrix\n",
    "    confmat = np.zeros((K, K))\n",
    "\n",
    "    # Populate the confusion matrix\n",
    "    for true_class in unique_classes:\n",
    "        true_class_index = class_to_index[true_class]\n",
    "        Ik = np.where(y_true == true_class)[0]\n",
    "        pred_classes_indices = [class_to_index[pred] for pred in y_pred[Ik]]\n",
    "        for pred_index in pred_classes_indices:\n",
    "            confmat[true_class_index, pred_index] += 1\n",
    "\n",
    "    # Normalize the rows of the confusion matrix to get probabilities\n",
    "    row_sums = confmat.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  # Prevent division by zero for classes not in y_true\n",
    "    confmat /= row_sums\n",
    "\n",
    "    # Calculate the conditional risk for each true category\n",
    "    R = np.dot(L, confmat.T).diagonal()\n",
    "\n",
    "    return R, confmat\n",
    "\n",
    "def compute_conditional_riskSalva(YR, Yhat, K, L): \n",
    "    '''\n",
    "    Function to compute the class-conditional risks.\n",
    "    Parameters\n",
    "    ----------\n",
    "    YR : DataFrame\n",
    "        Real labels.\n",
    "    Yhat : Array\n",
    "        Predicted labels.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : Array of floats\n",
    "        Conditional risks.\n",
    "    confmat : Matrix\n",
    "        Confusion matrix.\n",
    "    '''\n",
    "    Labels=[i for i in range(K)]\n",
    "    confmat=confusion_matrix(np.array(YR),np.array(Yhat),normalize='true',labels=Labels)\n",
    "    R=np.sum(np.multiply(L, confmat),axis=1)\n",
    "\n",
    "   # Is only the confuns \n",
    "    \n",
    "    return R, confmat\n",
    "\n",
    "def compute_conditional_riskCyprien(YR, Yhat, K, L): \n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    YR : DataFrame\n",
    "        Real labels.\n",
    "    Yhat : Array\n",
    "        Predicted labels.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    R : Array of floats\n",
    "        Conditional risks.\n",
    "    confmat : Matrix\n",
    "        Confusion matrix.\n",
    "    '''\n",
    "   \n",
    "    confmat = np.zeros((K, K))\n",
    "    R = np.zeros((1, K))\n",
    "    YR_liste= YR.values.tolist()\n",
    "    for k in range(0, K):\n",
    "        mk = YR_liste.count([k+1])\n",
    "        if mk > 0:\n",
    "            Ik = np.where(YR == (k + 1))\n",
    "            for l in range(0, K):\n",
    "                confmat[k,l] = sum(Yhat[Ik[0]]==l+1)/mk\n",
    "        R[0,k] = L[k, :].dot(confmat[k, :]) \n",
    "    \n",
    "    return R, confmat\n",
    "start=time.time()\n",
    "O1,O2=compute_conditional_riskCyprien(pd.DataFrame(y+1),y_pred+1,k,L)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "W1,W2=compute_conditional_riskWenlong(y,y_pred,k,L)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "S1,S2=compute_conditional_riskSalva(y,y_pred,k,L)\n",
    "print(time.time()-start)\n",
    "checkMatriz(S2,W2)\n",
    "checkarray(S1,W1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_piStarSalva(pHat, y_train, K, L, T, N, optionPlot, Box):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile in each class.\n",
    "    y_train : Dataframe\n",
    "        Real labels of the training set.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "    T : int\n",
    "        Number of discrete profiles.\n",
    "    N : int\n",
    "        Number of iterations in the projected subgradient algorithm.\n",
    "    optionPlot : int {0,1}\n",
    "        1 plots figure,   0: does not plot figure.\n",
    "    Box : Array\n",
    "        {'none', matrix} : Box-constraints on the priors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piStar : Array of floats\n",
    "        Least favorable priors.\n",
    "    rStar : float\n",
    "        Global risks.\n",
    "    RStar : Array of float\n",
    "        Conditional risks.\n",
    "    V_iter : Array\n",
    "        Values of the V function at each iteration.\n",
    "    stockpi : Array\n",
    "        Values of pi at each iteration.\n",
    "\n",
    "    \"\"\"\n",
    "    # IF BOX-CONSTRAINT == NONE (PROJECTION ONTO THE SIMPLEX)\n",
    "    if Box is None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "\n",
    "            mu_k = np.sum(L[:, np.argmin(lambd, axis=0)] * pHat, axis=1)\n",
    "            R[0,:] = mu_k\n",
    "            stockpi[:,n-1] = pi[0,:]\n",
    "            \n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_simplex_Condat(K, w)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    # IF BOX-CONSTRAINT\n",
    "    if Box is not None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:, t])\n",
    "                    mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "                R[0, k] = mu_k\n",
    "                stockpi[k, n - 1] = pi[0, k]\n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_onto_U(w, Box, K)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    return piStar, rStar, RStar, V_iter, stockpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_piStarWenlong(pHat, y_train, K, L, T, N, optionPlot, Box):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    pHat : Array of floats\n",
    "        Probability estimate of observing the features profile in each class.\n",
    "    y_train : Dataframe\n",
    "        Real labels of the training set.\n",
    "    K : int\n",
    "        Number of classes.\n",
    "    L : Array\n",
    "        Loss Function.\n",
    "    T : int\n",
    "        Number of discrete profiles.\n",
    "    N : int\n",
    "        Number of iterations in the projected subgradient algorithm.\n",
    "    optionPlot : int {0,1}\n",
    "        1 plots figure,   0: does not plot figure.\n",
    "    Box : Array\n",
    "        {'none', matrix} : Box-constraints on the priors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    piStar : Array of floats\n",
    "        Least favorable priors.\n",
    "    rStar : float\n",
    "        Global risks.\n",
    "    RStar : Array of float\n",
    "        Conditional risks.\n",
    "    V_iter : Array\n",
    "        Values of the V function at each iteration.\n",
    "    stockpi : Array\n",
    "        Values of pi at each iteration.\n",
    "\n",
    "    \"\"\"\n",
    "    # IF BOX-CONSTRAINT == NONE (PROJECTION ONTO THE SIMPLEX)\n",
    "    if Box is None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:, t])\n",
    "                    mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "                R[0, k] = mu_k\n",
    "                stockpi[k, n - 1] = pi[0, k]\n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_simplex_Condat(K, w)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    # IF BOX-CONSTRAINT\n",
    "    if Box is not None:\n",
    "        pi = compute_pi(y_train, K).reshape(1, -1)\n",
    "        rStar = 0\n",
    "        piStar = pi\n",
    "        RStar = 0\n",
    "\n",
    "        V_iter = []\n",
    "        stockpi = np.zeros((K, N))\n",
    "\n",
    "        for n in range(1, N + 1):\n",
    "            # Compute subgradient R at point pi (see equation (21) in the paper)\n",
    "            lambd = np.dot(L, pi.T * pHat)\n",
    "            R = np.zeros((1, K))\n",
    "            for k in range(0, K):\n",
    "                mu_k = 0\n",
    "                for t in range(0, T):\n",
    "                    lbar = np.argmin(lambd[:, t])\n",
    "                    mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "                R[0, k] = mu_k\n",
    "                stockpi[k, n - 1] = pi[0, k]\n",
    "            r = compute_global_risk(R, pi)\n",
    "            V_iter.append(r)\n",
    "            if r > rStar:\n",
    "                rStar = r\n",
    "                piStar = pi\n",
    "                RStar = R\n",
    "                # Update pi for iteration n+1\n",
    "            gamma = 1 / n\n",
    "            eta = np.maximum(float(1), np.linalg.norm(R))\n",
    "            w = pi + (gamma / eta) * R\n",
    "            pi = proj_onto_U(w, Box, K)\n",
    "\n",
    "        # Check if pi_N == piStar\n",
    "        lambd = np.dot(L, pi.T * pHat)\n",
    "        R = np.zeros((1, K))\n",
    "        for k in range(0, K):\n",
    "            mu_k = 0\n",
    "            for t in range(0, T):\n",
    "                lbar = np.argmin(lambd[:, t])\n",
    "                mu_k = mu_k + L[k, lbar] * pHat[k, t]\n",
    "            R[0, k] = mu_k\n",
    "            stockpi[k, n - 1] = pi[0, k]\n",
    "        r = compute_global_risk(R, pi)\n",
    "        if r > rStar:\n",
    "            rStar = r\n",
    "            piStar = pi\n",
    "            RStar = R\n",
    "\n",
    "        if optionPlot == 1:\n",
    "            graph_convergence(V_iter)\n",
    "\n",
    "    return piStar, rStar, RStar, V_iter, stockpi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phat=compute_pHat(XD, y, k, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9135763645172119\n",
      "39.8628044128418\n"
     ]
    }
   ],
   "source": [
    "N=10000\n",
    "start=time.time()\n",
    "W1,W2,W3,W4,W5=compute_piStarWenlong(Phat, y, k, L, t, N, 0, None)\n",
    "print(time.time()-start)\n",
    "start=time.time()\n",
    "S1,S2,S3,S4,S5=compute_piStarSalva(Phat, y, k, L, t, N, 0, None)\n",
    "print(time.time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "-1.1102230246251565e-16\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "checkarray(W1[0],S1[0])\n",
    "print(W2-S2)\n",
    "checkarray(W3[0],S3[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.85572139, 0.84924623, 1.        , 0.87939698, 0.79104478,\n",
       "        1.        , 0.86567164, 0.87437186, 0.90640394, 0.7960199 ,\n",
       "        0.8241206 , 1.        , 0.87939698, 1.        , 1.        ,\n",
       "        0.89949749, 1.        , 0.85353535, 0.85929648, 0.77889447,\n",
       "        1.        , 0.90640394, 0.80808081, 0.86138614, 0.87562189,\n",
       "        0.75124378, 1.        , 0.87192118, 1.        , 1.        ,\n",
       "        1.        , 0.88613861, 0.90049751, 1.        , 1.        ,\n",
       "        0.84343434, 0.81094527, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.73762376, 0.73869347, 1.        , 0.81818182]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9173445373685474"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
